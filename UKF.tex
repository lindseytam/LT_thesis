\chapter{Unscented Kalman Filters}
\label{Unscented Kalman Filters}

The Unscented Kalman Filter (UKF) is another nonlinear version of the Kalman Filter developed to address the shortcomings of the EKF. For instance, as opposed to using the Jacobean to linearly approximate around a single point, the UKF uses the Unscented Transform (UT) to approximate around multiple points, known as sigma points. The UT is a method of approximating probability distributions that have undergone a non linear transformation using limited statistics. The UT involves using these sigma points, which are represented in a Sigma Point Matrix, to represent the normal distribution of the data. The covariance and weights of these sigma points are calculated. These sigma points undergo a non-linear transformation, resulting in a posterior distribution that is not normal \cite{inbook, Wan01theunscented} . We are able to approximate the normal distribution of the posterior distribution using the weights and covariance that were calculated prior to the transformation. This process enables the Kalman Filter to be applied to more complex non linear problems. \\ \\
Unlike the Kalman Filter and the Extended Kalman Filter, the UKF also has a set of parameters. Explanations of each parameter and their default values can be found in the chart below. For the UKF, parameters are necessary for controlling the spread of sigma points. This was not needed for the EKF, since the EKF was only linearizing around the mean.
\textcolor{red}{expand here on how to tune parameters.} \\ \\
The term 'unscented' was arbitrarily coined by the developer of the UKF, Jeffrey Uhlmann. In an interview, he shares:
\begin{displayquote}
"Initially I only referred to it as the “new filter.” Needing a more specific name, people in my lab began referring to it as the “Uhlmann filter,” which obviously isn’t a name that I could use, so I had to come up with an official term. One evening everyone else in the lab was at the Royal Opera House, and as I was working I noticed someone’s deodorant on a desk. The word “unscented” caught my eye as the perfect technical term. At first people in the lab thought it was absurd—which is okay because absurdity is my guiding principle—and that it wouldn’t catch on. My claim was that people simply accept technical terms as technical terms: for example, does anyone think about why a tree is called a tree?"
\end{displayquote}

\section{Unscented Kalman Filter Algorithm}
\begin{center}
    
\centering
\begin{tabular}{ |p{2cm}||p{5cm}|p{2cm}| }
    \hline
    \multicolumn{3}{|c|}{Variables in the Unscented Kalman Filter } \\ 
    \hline
    Variable & Description & Dimensions \\
    \hline
    x & Vector containing state variables & $d_x \times 1 $\\ 
    $\chi $& Sigma Point Matrix &$ d_x \times (2 d_x + 1) $\\
    f & Nonlinear tate function & $d_x \times d_x $  \\ 
    h & Nonlinear observation function & $d_u \times d_x$\\
    u & Input vector  & $d_u \times 1$\\
    P & Covariance matrix & $d_x \times d_x $  \\
    Q & Process noise covariance & $d_x \times d_x $  \\
    R & Measurement noise covariance & $d_y \times d_y $  \\
     $W^{(m)}$ & Weight for state variable (mean) & scalar \\
    $W^{(c)}$ & Weight for covariance & scalar \\
    \hline
\end{tabular} 
\end{center}
\begin{center}
\begin{tabular}{ |p{1cm}||p{5cm}|p{2cm}| p{1cm}| }
    \hline
    \multicolumn{4}{|c|}{Parameters in the Unscented Kalman Filter } \\ 
    \hline
     & Description & Bounds & Default \\
    \hline
    $\alpha$ & Controls spread of sigma points & $0 < \alpha \leq 1$ & $.001$\\
    $\beta$ & Adjust sigma point weight & $\beta \geq 0$ & 2\\
    $\kappa $ & Sigma point weighting constant & $0 \leq \kappa \leq 3^{*}$  & 0 \\
    \hline
\end{tabular}
\end{center}
* Others use $\kappa  = 3 - d_x$ 

\begin{enumerate}
    \item Initialize state vector and covariance
    \begin{align*}
        \hat{x}_{0} &= \mathbb{E}[x_{0}] 
       \end{align*}
        \begin{align*}
        P_{x_{0}} &= \mathbb{E}[(x_{0}-\hat{x}_{0})(x_{0}-\hat{x}_{0})^{T}] 
    \end{align*}
    This step is the exact same as the first step of the KF and the EKF. We need to initialize these values so the model can begin generating and correcting predictions.
    
        \item Calculate sigma points \\ \\
        Sigma points characterize the distribution of the data. The number of sigma points is deterministic and depends on the dimensions of the system. In general, a UFK will have  $2 \cdot d_x$ + 1 sigma points, where $d_x$ represents the dimension of the state vector \cite{inbook, inproceedings, Wan01theunscented}.  We use the equation below to generate a scalar value that determines how spread out the sigma points are from the mean. 
         \begin{align*}
        \lambda = \alpha^{2}(d_{x}+\kappa)-d_{x} 
         \end{align*}
         $\alpha$ and $\kappa$ are both parameters that control for the spread of sigma points around the mean value of the state. The spread of the sigma points is proportional to $\alpha$ . For both $\alpha$ and $\kappa $,  the smaller the values are, the closer the sigma points are to the mean.\\ \\
       $\beta$ is a parameter that uses information regarding state distribution to adjust sigma points. $\beta$ has a default value of 2 if the data is Gaussian. 
    \begin{align*}
        \chi_{\ 0,k-1} &= \hat{x}_{k-1} 
     \end{align*}
             Since the goal is to characterize the distribution, set one of these sigma points to the mean. Half of the remaining points will be smaller than the mean and the other half will be larger than the mean.
     \begin{align*}
        \chi_{\ i,k-1} &= \hat{x}_{k-1} \pm \bigg(\sqrt{(d_{x}+\lambda )P_{x_{k-1}}}\bigg)_{i} \quad \quad \quad i=1,\dots,2 d_x + 1
        \end{align*}
        The square root of a matrix, call it $A$ satisfies the following condition: $A = B^2$. Note that $(\sqrt{(d_{x}+\lambda)P_{x_{k-1}}})$ is a matrix, and the $i$ subscript is the $i^{th}$ column of the matrix. Also note that $\chi_{\ i,k-1}$ is the $i^{th}$ column of the sigma point matrix at time $k-1$.
       

        \item Calculate the weights for each sigma point \\ \\
        Weights are scalars used to calculate posterior sigma points after they have undergone a nonlinear transformation. The weights are later used to approximate Gaussian mean and covariance. Weights can have positive or negative values, but will ultimately sum to 1 \cite{article6}. The subscript of the weight indicates which sigma point the weight is for. 
        \begin{align*}
        W^{(m)}_{0} = \frac{\lambda}{d_{x}+ \lambda} 
         \end{align*}
         This is the weight for the mean of the 0th sigma point. 
        \begin{align*}
        W^{(c)}_{0} = \frac{\lambda}{d_{x}+ \lambda} + (1 - \alpha^{2} + \beta) 
         \end{align*}
         This is the weight for the covariance of the 0th sigma point. 
        \begin{align*}
        W^{(m)}_{i} = W^{(c)}_{i} = \frac{\lambda}{2(d_{x}+ \lambda) } \quad \quad \quad i=1,\dots,2d_{x}
            \end{align*}
           The rest of the weights for the means and covariances of the other $2 d_x$ sigma points is calculated above.
      
           
        \item Generate a prediction
        \begin{align*}
        \chi_{k | k - 1} = f(\chi)
        \end{align*}
        The equation above performs nonlinear transformation $f$ on the sigma points. Though $\chi$ has a Gaussian distribution,  $ \chi_{k | k - 1} $ does not because it has been transformed by the nonlinear state function $f$.
        \begin{align*}
        \hat x_{k | k-1} = \sum^{2d_x}_{i = 0} W_i^{(m)} \chi_{i, k | k - 1}
        \end{align*}
        Now, the new estimate of the state variables is conditioned on the last estimate. The weights are included in this step in order to approximate the Gaussian distribution.
        \begin{align*}
        P_{x, k | k-1} = \sum^{2d_x}_{i = 0} W_i^{(c)} (\chi_{i, k | k - 1} -  \hat x_{i, k | k - 1} )(\chi_{i, k | k - 1} -  \hat x_{i, k | k - 1} )^T + Q
        \end{align*} 
        The posterior covariance matrix for the state variable is necessary for updating the state covariance later on (step 9). Recall that Q is process noise, which provides the error in our model $f$.
        \item Calculate posterior sigma points \\ \\
        By now, the prediction step has concluded and the model begins the process of correction. Calculation of the posterior (also called augmented) sigma points is necessary for interpreting the distribution. From the non linear transformation above, the outputs are not Gaussian. Using the weights calculated in step 3, we are able to approximate the Gaussian distribution of the transformed sigma points.
         \begin{align*}
        \chi^{(aug)}_{0, k|k-1} = \hat x_{k|k-1}
        \end{align*}
         \begin{align*}
        \chi^{(aug)}_{ i,k |k-1} &= \hat{x}_{k |k-1} \pm \bigg(\sqrt{(d_{x}+\lambda)P_{x_{k |k-1}}} \bigg)_{i} \quad \quad \quad  i=1,\dots,2d_{x}
        \end{align*}
        The calculation is the same process as in step 2. Recall that $\lambda$ was calculated in step 2. Since it is not time dependent, we can use the same $\lambda$ value used earlier.
        
        \item Calculate transformed output 
         \begin{align*}
       \mathcal{Y}_{k|k-1} = h(\chi^{(aug)}_{k|k-1}) 
       \end{align*}
       \begin{align*}
       y_{k|k-1} = \sum^{2d_x}_{i = 0} W_i^{(m)}  \mathcal{Y}_{i, k | k - 1}
       \end{align*}
       Similar to the KF and EKF, use observation function $h$ to convert the sigma point matrix into ta form that can be compared to the actual measurements of the system $y_k$. Since this form is not Gaussian, use weights to interpret the results.
       
       \item Calculate Kalman Gain \\ \\
       Unlike previous versions of the KF, in addition to calculating the covariance of the state variables, calculations is also done for the covariance of observations  $P_{y}$ and for state variables with observations $P_{xy}$. When generating the covariance matrix for y the covariance of measurement noise is added. 
        \begin{align*}
       P_{y, k | k-1} = \sum^{2d_x}_{i = 0} W_i^{(c)} (\mathcal{Y}_{i, k | k - 1} -   y_{i, k | k - 1} )(\mathcal{Y}_{i, k | k - 1} -  y_{i, k | k - 1} )^T + R
       \end{align*}
        \begin{align*}
       P_{xy, k | k-1} = \sum^{2d_x}_{i = 0} W_i^{(c)} (\chi^{(aug)}_{i, k | k - 1} -  \hat x_{i, k | k - 1} )(\mathcal{Y}_{i, k | k - 1} -  y_{i, k | k - 1} )^T 
       \end{align*}
       \begin{align*}
       K_k = P_{xy, k | k-1} (P_{y, k | k-1}) ^{-1}
       \end{align*}
        
      \item Correct the prediction      
      \begin{align*}
       \hat x_{k} = \hat x_{k|k-1} + K_k(y_k - y_{k|k-1})
        \end{align*}
        Similar to the KF and EKF, the correction step follows the same equation. 
      
      
      \item Update the covariance matrix 
       \begin{align*}
       P_{x, k} = P_{x, k|k-1} -K_k (P_{y, k | k-1} ) {K_k}^T     
       \end{align*}     
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
            
    
    
        
\end{enumerate}