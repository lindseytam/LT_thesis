\chapter{Unscented Kalman Filters}
\label{Unscented Kalman Filters}

The Unscented Kalman Filter (UKF) is another nonlinear version of the Kalman Filter. The UKF was designed to address the problems of the EKF. For instance, as opposed to using the Jacobean to linearly approximate around a single point, the UKF uses the Unscented Transform (UT) to approximate around multiple points, known as sigma points. The UT is a method of approximating probability distributions that have undergone a non linear transformation using limited statistics. The UT involves using these sigma points, which are represented in a Sigma Point Matrix, to represent the normal distribution of the data. The covariance and weights of these sigma points are calculated. These sigma points undergo a non-linear transformation, resulting in a posterior distribution that is not normal \cite{inbook, Wan01theunscented} . We are able to approximate the normal distribution of the posterior distribution using the weights and covariance that were calculated prior to the transformation. This process enables the Kalman Filter to be applied to more complex non linear problems. \\ \\
Unlike the Kalman Filter and the Extended Kalman Filter, the UKF also has a set of parameters. Explanations of each parameter and their default values can be found in the chart below. For the UKF, parameters are necessary for controlling the spread of sigma points. This was not needed for the EKF, since the EKF was only linearizing around the mean.
\textcolor{red}{expand here on how to tune parameters.}

\section{Unscented Kalman Filter Algorithm}
\begin{center}
    
\centering
\begin{tabular}{ |p{2cm}||p{5cm}|p{2cm}| }
    \hline
    \multicolumn{3}{|c|}{Variables in the Unscented Kalman Filter } \\ 
    \hline
    Variable & Description & Dimensions \\
    \hline
    x & Vector containing state variables & $d_x \times 1 $\\ 
    $\chi $& Sigma Point Matrix &$ d_x \times (2 d_x + 1) $\\
    F & State function & $d_x \times d_x $  \\ 
    H & Observation function & $d_y \times d_x$\\
    G & Input matrix & $d_x \times d_u$\\
    u & Input vector  & $d_u \times 1$\\
    P & Covariance matrix & $d_x \times d_x $  \\
    w & Process Noise Vector & $d_x \times 1$\\
    \hline
\end{tabular} 
\end{center}
\begin{center}
\begin{tabular}{ |p{1cm}||p{5cm}|p{2cm}| p{1cm}| }
    \hline
    \multicolumn{4}{|c|}{Parameters in the Unscented Kalman Filter } \\ 
    \hline
     & Description & Bounds & Default \\
    \hline
    $\alpha$ & Controls spread of sigma points & $0 < \alpha \leq 1$ & $.001$\\
    $\beta$ & Adjust sigma point weight & $\beta \geq 0$ & 2\\
    $\kappa $ & Sigma point weighting constant & $0 \leq \kappa \leq 3$  & 0\\
    \hline
\end{tabular}
\end{center}

\begin{enumerate}
    \item Initialize state vector and covariance
    \begin{align*}
        \hat{x}_{0} &= \mathbb{E}[x_{0}] 
       \end{align*}
        \begin{align*}
        P_{x_{0}} &= \mathbb{E}[(x_{0}-\hat{x}_{0})(x_{0}-\hat{x}_{0})^{T}] 
    \end{align*}
    This step is the exact same as the first step of the KF and the EKF. We need to initialize these values so the model can begin generating and correcting predictions.
        \item Calculate the weights for each sigma point 
    \begin{align*}
        \lambda = \alpha^{2}(d_{x}+\kappa)-d_{x} 
         \end{align*}
        \begin{align*}
        W^{(m)}_{0} = \frac{\lambda_{x}}{d_{x}+ \lambda} 
         \end{align*}
        \begin{align*}
        W^{(c)}_{0} = \frac{\lambda}{d_{x}+ \lambda} + (1 - \alpha^{2} + \beta) 
         \end{align*}
        \begin{align*}
        W^{(m)}_{i} = W^{(c)}_{i} = \frac{\lambda}{2(d_{x}+ \lambda) } \quad \quad \quad i=1,\dots,2d_{x} \\
            \end{align*}
       $\alpha$ and $\kappa$ are both parameters that control for the spread of sigma points around the mean value of the state. The spread of the sigma points is proportional to $\alpha$ . For both $\alpha$ and $\kappa $,  the smaller the values are, the closer the sigma points are to the mean.\\ \\
       $\beta$ uses information regarding state distribution to adjust sigma points. $\beta$ has a default value of 2 if the data is Gaussian. \\ \\
    Weights are scalars used to calculate posterior sigma points after they have undergone a nonlinear transformation. The weights are used to approximate Gaussian mean and covariance. Weights can have positive or negative values, but will ultimately sum to 1 \cite{article6}
    
    \item Calculate the augmented weights for each sigma point 
    \begin{align*}
        W^{\text{aug}(m)}_{0} = \frac{\lambda}{2d_{x}+ \lambda } 
        \end{align*}
         \begin{align*}
        W^{\text{aug}(c)}_{0} = \frac{\lambda}{2d_{x}+ \lambda} + (1 - \alpha^{2} + \beta) 
        \end{align*}
         \begin{align*}
        W^{\text{aug}(m)}_{i} = W^{\text{aug,}(c)}_{i} = \frac{\lambda}{2(2d_{x}+ \lambda) } \quad \quad \quad i=1,\dots,4d_{x} 
    \end{align*}
    Augmented weights will be used later for measurement forecast and calculating Kalman Gain.
    \item Calculate sigma points
    \begin{align*}
        \chi_{\ 0,k-1} &= \hat{x}_{k-1} 
     \end{align*}
     \begin{align*}
        \chi_{\ i,k-1} &= \hat{x}_{k-1} + (\sqrt{(n_{x}+\lambda)P_{x_{k-1}}})_{i} \quad \quad \quad i=1,\dots,d_{x} 
        \end{align*}
        \begin{align*}
        \chi_{\ i,k-1} &= \hat{x}_{k-1} - (\sqrt{(n_{x}+\lambda)P_{x_{k-1}}})_{i} \quad \quad \quad  i=d_{x}+1,\dots,2n_{x},
    \end{align*}
    Sigma points characterize the distribution of the data. The number of sigma points is deterministic and depends on the dimensions of the system. In general, a UFK will have 2 $d_x$ + 1 sigma points, where $d_x$ represents the dimension of the state vector \cite{inbook, inproceedings, Wan01theunscented}. One of these sigma points will be the mean. Half of the remaining points will be smaller than the mean and the other half will be larger than the mean. Determining how spread each of these points are from the mean depends on parameters $\alpha$ and $\kappa$.

    \item Perform a non-linear transformation on the sigma points
    \begin{align*}
        \hat{w}_{k} &= \hat{w}_{k-1} 
        \end{align*}
        \begin{align*}
        \chi^{*}_{\ i,k|k-1} &= f(\chi_{\ i,k-1},\hat{w}_{k-1})
    \end{align*}
    \item Calculate the mean and covariance of the transformed sigma points
    \begin{align*}
        \hat{x}^{*}_{\ k|k-1} &= \sum_{i=0}^{2n_{x}} W^{(m)}_{x,i} \chi^{*}_{\ i, k|k-1} 
        \end{align*}
        \begin{align*}
        P_{x_{k|k-1}} &=  \sum_{i=0}^{2n_{x}} W^{(c)}_{x,i} (\chi^{*}_{\ i, k|k-1} - \hat{x}^{*}_{\ k|k-1})(\chi^{*}_{\ i, k|k-1} - \hat{x}^{*}_{\ k|k-1})^{T} + R_{v} % Q_k is same as R_{v} (Process Noise Covariance)
    \end{align*}
    \item Re-calculate sigma points
    \begin{align*}
        \chi_{\ i, k|k-1} = \chi^{*}_{\ i,k|k-1} \quad \quad \quad  i=0,...,2d_{x} 
         \end{align*}
         \begin{align*}
        \chi_{\ i, k|k-1} = \chi^{*}_{\ 0,k|k-1} + (\sqrt{(n_{x}+\lambda_{x})P_{x_{k|k-1}}})_{i}  \quad \quad \quad i=2d_{x}+1,\dots,3d_{x} 
         \end{align*}
         \begin{align*}
        \chi_{\ i, k|k-1} = \chi^{*}_{\ 0,k|k-1} - (\sqrt{(n_{x}+\lambda_{x})P_{x_{k|k-1}}})_{i}  \quad \quad \quad  i=3d_{x}+1,\dots,4n_{x}
    \end{align*}
    \item Generate prediction
    \begin{align*}
        \hat{x}_{k|k-1} = \sum_{i=0}^{4n_{x}} W^{\text{aug,}(m)}_{x,i} \chi_{\ i, k|k-1} 
        \end{align*}
         \begin{align*}
        \mathcal{Y}_{k|k-1} = h(\chi_{k|k-1},\hat{w_{k}}) 
        \end{align*}
         \begin{align*}
        \hat{y}_{k|k-1} = \sum_{i=0}^{4n_{x}} W^{\text{aug,}(m)}_{x,i} \mathcal{Y}_{\ i, k|k-1}
    \end{align*}
    \item Calculate Kalman Gain
    \begin{align*}
        P_{y_{k}} = \sum_{i=0}^{4n_{x}} W^{\text{aug,}(c)}_{x,i} (\mathcal{Y}_{i, k|k-1} - \hat{y}_{k|k-1})(\mathcal{Y}_{i, k|k-1} - \hat{y}_{k|k-1})^{T} + R_{n}   %R_k is the same as R_{n} (Measurement Noise Covariance)
        \end{align*}
        \begin{align*}
        P_{x_{k}y_{x}} = \sum_{i=0}^{4n_{x}} W^{\text{aug,}(c)}_{x,i} (\chi_{\ i, k|k-1} - \hat{x}^{*}_{\ k|k-1})(\mathcal{Y}_{i, k|k-1} - \hat{y}_{k|k-1})^{T} 
        \end{align*}
        \begin{align*}
        K_{k} = P_{x_{k}y_{x}}P^{-1}_{y_{k}}
    \end{align*}
    \item Update Estimate and state covariance
   \begin{align*}
        P_{x_{k}} = P_{x_{k|k-1}} - K_{k}P_{y_{k}}K^{-1}_{k} 
   \end{align*}
    \begin{align*}
        \hat{x}_{k} = \hat{x}_{k|k-1} + K_{k}(y_{k}-\hat{y}_{k|k-1}),
    \end{align*}
    
\end{enumerate}